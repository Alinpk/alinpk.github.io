[{"content":"NFS 背景 NFS（Network FileSystem）是在工作中搭建多机共享目录所使用的一个机制。在使用过程中，只是模糊的了解了一下其背后的机制，如果后续有继续学习的话，则会基于当前内容加以跟新完善。\n所以目前的内容主要是集中在简单的过一下他的机制，具体的操作步骤是什么样的，还有自己遇到的一丢丢问题。但实际的介绍过程会先演示一下整个操作过程，之后再去解释这些过程的内容以及机制。\n过程 首先是介绍一下演示环境，两台主机都是Linux系统，彼此之间在同一个网段内部，一个是基于arm的架构，另一个是基于x86的架构。其中文件本体是放在x86系统之上的。（之所以先介绍实验环境，是因为我也是初次使用，脱离了这套环境可能还会遇到更多的问题，比如如果不是同一个网段，是不是要考虑各种ip映射穿透一类的问题，如果两台主机一个linux另一个window，那有一些fs仅在某个机子架构上支持而另一个不被，会不会有什么坑）\n接着说一下大致的操作\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # work_directory is our testing path(abs path) # x86(server) # 先确认一下是不是安装好了nfs先关的包 sudo apt-get install nfs-kernel-server mkdir -p work_diretory chown nobody:nogroup work_directory # 这个是在规范标准中说明的供NFS使用的user\u0026amp;grp chmod 755 work_directory # client_ip是可以使用通配符的，后面括号里的参数可以使用时再查阅一下，主要是权限/读写同步异步/用户映射（参考1） sudo echo \u0026#34;work_directory client_ip(rw, sync, no_subtree_check)\u0026#34; \u0026gt;\u0026gt; /etc/exports sudo exportfs -a sudo systemctl restart nfs-kernel-server # sudo exportfs -v #检查一些状态 # arm(client) sudo apt-get install nfs-common mkdir -p local_dircetory # 可以显示-t nfs，-o应该是-option，里面可以指明传输层协议以及一些细节(参考2) sudo mount server_ip:work_directory local_directory -o tcp # df -h #检查一下 原理 NFS，网络文件系统，那就可以拆成两部分理解，网络+文件系统，首先是用起来，用户的感知上来说是无异于普通文件系统的（不过估计还是有微妙差异，网络崩了怎么处理也是需要再查阅一下文档才懂），另一部分则是他还是符合一般的网络架构，p2p 或是c/s。不过有了上面的展示其实就可以猜到，这应该是一款基于cs模式的框架。因为撇去在server端要严格核对client的名单外的场景外，我们的server是完全可以不必去理会有哪些client，”我在这里，你们来！“。反倒是client，也就是上面的arm，是需要直到我要连的具体是哪个ip的哪个文件夹。\n不过看到这里的时候，就产生了第一个疑问了，网络通信，那我怎么知道要怎么通信，tcp/udp，还是nfs建立于更底层的协议？不过这点显然不太可能，那估计就是有一个常用的端口了，但如果我直接朝固定端口连过去，那岂不是我两台主机间已经限定死了只有一个share directory?显然不是。所以实际架构是如下图所示： 这里的mapping和那个daemon就是我们一开始拉起的nfs-kernel-server啦。一个负责分配端口号，另一个则是管理我们的权限问题。大体流程就如上所示。有了上面这个机制，就可以自由的建立好多的共享文件了。\n以上便是我目前所了解到的大致原理，至于NFS的可靠性，一致性，性能等问题如何保证，我觉得这点很重要，因为分布式文件系统无法绕开这几个问题，但是既然它是写入标准的NFS，自然在这几点会有一个合理的均衡，而分布式的学习本来也是在后续计划中，因此这里就先不深究了嘻嘻。 (btw中间创建的时候发现有些地方总是会permission denied，是因为并非所有文件系统类型都支持nfs的，遇到了这点可以去手册查一下，还有就是软链接+nfs还是挺舒服的)\n参考连接 configuring the nfs server\ncommon nfs mount options\n","date":"2024-08-29T01:55:16+08:00","permalink":"https://example.com/p/nfs/","title":"nfs"},{"content":"brief introduce 简单的规划一下目前的几个事：\n网络技术内幕 协程框架的编写，计划待定 部署一个gitlab 看看jeckins怎么结合到gitlab上 reading ipc shmipc jonasmr/microprofile\n","date":"2024-08-26T01:31:39+08:00","permalink":"https://example.com/p/first-plan/","title":"first plan"},{"content":" image container repository\nimage 那么镜像到底是什么呢？Docker 镜像可以看作是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）\nContainer（容器） 容器（Container）的定义和镜像（Image）几乎一模一样，也是一堆层的统一视角，唯一区别在于容器的最上面那一层是可读可写的。\nRepository（仓库） Docker 仓库是集中存放镜像文件的场所。镜像构建完成后，可以很容易的在当前宿主上运行。但是， 如果需要在其他服务器上使用这个镜像，我们就需要一个集中的存储、分发镜像的服务，Docker Registry（仓库注册服务器）就是这样的服务。有时候会把仓库（Repository）和仓库注册服务器（Registry）混为一谈，并不严格区分。\n仓库又可以分为两种形式：Public（公有仓库）Private（私有仓库）Docker Registry 公有仓库是开放给用户使用、允许用户管理镜像的 Registry 服务。一般这类公开服务允许用户免费上传、下载公开的镜像，并可能提供收费服务供用户管理私有镜像。除了使用公开服务外，用户还可以在本地搭建私有 Docker Registry。Docker 官方提供了 Docker Registry 镜像，可以直接使用做为私有 Registry 服务。\narchitecture Docker 的核心组件包括：\nDocker Client Docker Daemon Docker Image Docker Registry Docker Container\nDocker Client\n简单来说就是CLI，可以构建、运行、停止运用程序，还可以远程和Docker_Host进行交互。 最常用的Docker客户端就是docker命令\nDocker Daemon\nDocker Daemon是服务器组件，在Linux上以守护进程的形式运行，也是Docker的核心。 Docker Daemon 可以认为是通过 Docker Server 模块接受 Docker Client 的请求，并在 Engine 中处理请求，然后根据请求类型，创建出指定的 Job 并运行。Docker Daemon 运行在 Docker Host 上，负责创建、运行、监控容器，构建、存储镜像。\n以下是Docker Daemon的架构图 运行过程的作用有以下几种可能：\n向 Docker Registry 获取镜像。 通过 GraphDriver 执行容器镜像的本地化操作。 通过 NetworkDriver 执行容器网络环境的配置。 通过 ExecDriver 执行容器内部运行的执行工作。\n链接：https://zhuanlan.zhihu.com/p/600034612\n","date":"0001-01-01T00:00:00Z","permalink":"https://example.com/p/note/","title":"note"},{"content":"working note __stdcall 1.definition\n__stdcall 调用约定用于调用 Win32 API 函数。 被调用方将清理堆栈，以便让编译器生成 vararg 函数 __cdecl。 使用此调用约定的函数需要一个函数原型。 __stdcall 修饰符是 Microsoft 专用的。\n2.why\n先感受一下两种风格的效果\n传统上的约定应该是__cdecl，这个约定是函数传递的参数，应该由调用方去清理，从汇编角度看，类似于：\n1 2 3 4 5 6 /* example of __cdecl */ push arg1 push arg2 push arg3 call function add esp,12 ; effectively \u0026#34;pop; pop; pop\u0026#34; __stdcall的约定风格是在Win32 API函数的标准约定，做法如下:\n1 2 3 4 5 /* example of __stdcall */ push arg1 push arg2 push arg3 call function // no stack cleanup - callee does this 这样就有比较只管的感受了，从汇编码的角度来说，__stdcall把清理的职责归属到了callee，这样在大型项目中，可能会编译出更小的可执行程序。 另一方面，如果使用过C风格的变长参数便可以，虽然我们在语义层面上可以用类似于argc,argv的模式去明确参数的数量规则，但编译器层面去统一的约束这点是很困难的，所以__stdcall是不支持这种风格的边长参数的。\n这里给出win32流行的一些调用约定（实际跨平台项目我们一般会在非win环境定义对应的空白宏\n__stdcall, 以相反的顺序（从右到左）将参数压入堆栈 __cdecl, 以相反的顺序（从右到左）将参数压入堆栈 __clrcall, 按顺序（从左到右）将参数加载到 CLR 表达式堆栈中。 __fastcall, 存储在寄存器中，然后压入堆栈 __thiscall, 入栈；此指针存储在 ECX 中 attribute visibility(default) 可以用来定义一些符号是不是可见，可以用来发布api\n","date":"0001-01-01T00:00:00Z","permalink":"https://example.com/p/note/","title":"note"},{"content":" title: \u0026ldquo;note\u0026rdquo; draft: true categories: - unarchives tags: - draft 杂七杂八 进程调度相关 常见算法： 1. 实时进程调度： SCHED_FIIFO, SCHED_RR 2. 普通进程调度算法：CFS\n对应调度器的一些行为： 优先级上，对于实时进程，高优先级的会持续占用cpu（但一般会默认留下5%的资源给普通进程） 对于CFS，其一个重要的运行时参数是所谓的vruntime，其公平策略的核心就是保障每个进程在vruntime的数值上尽可能的相同。\n我们启动进程时设定的nice值越大，相同时间下vruntime增长的速度就越慢。\n此外CFS的公平机制可以是基于同一个CGROUP下的线程而言的，更具体的说：在cpu.share一致的场景下，CFS会尽可能的让每个thread的vruntime一致，这样会导致同一个CGROUP下的进程开越多的CPU就能获获取到更多的CPU时间片\n对于CGROUP资源调控可以通过对cgoup.procs和cpu.cfs_quota_us进行配置实现\n","date":"0001-01-01T00:00:00Z","permalink":"https://example.com/p/note/","title":"note"}]